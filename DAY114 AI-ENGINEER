ğŸš€ Day 114 of my AI Engineer Journey

Missing data isnâ€™t just an â€œannoyanceâ€â€¦ sometimes itâ€™s the hardest puzzle in ML. ğŸ§©

Today I leveled up from univariate imputation â†’ into multivariate imputation (using all columns to fill missing values, not just the one).

Hereâ€™s what I explored ğŸ‘‡

1ï¸âƒ£ KNN Imputer (K-Nearest Neighbors)

Finds the â€œnearest rowsâ€ (neighbors) using Euclidean distance.

Fills missing values with neighbor(s) â†’ exact value (k=1) or average (k>1).

âœ… Super accurate

âŒ Slow & memory-heavy (needs training data at inference)

2ï¸âƒ£ Iterative Imputer (MICE)

Multivariate Imputation by Chained Equations.

Predicts missing values of one column using all other columns.

Works iteratively until predictions stabilize.

âœ… Very powerful (captures relationships between features)

âŒ Computationally expensive

ğŸ’¡ My big takeaway:
Univariate methods are quick band-aids. But real-world datasets need smarter imputation that respects feature relationships. Accuracy often comes at the cost of compute â€” so thereâ€™s always a trade-off.

ğŸ‘‰ Question for the pros:
When deploying ML models, do you prefer lightweight imputations (mean/mode) for speed, or heavy ones (KNN, MICE) for accuracy?

#MachineLearning #DataScience #AI #Python #LearningInPublic
