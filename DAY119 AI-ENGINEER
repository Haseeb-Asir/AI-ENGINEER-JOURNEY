ğŸš€ Day 119 of my AI Engineer Journey

Yesterday, PCA math made my head explode. ğŸ¤¯
Todayâ€¦ it finally clicked. ğŸ‰

Hereâ€™s what I cracked:

Covariance matrices tell us about the spread of features + relationships between them.

PCA finds eigenvectors & eigenvalues of the covariance matrix.

The eigenvectors (unit vectors) with the largest eigenvalues become the principal components (the new axes).

Eigenvalues = how much variance each component explains.

By keeping only the top k components â†’ we reduce dimensionality while keeping most of the info.


ğŸ”‘ Lesson learned:
PCA isnâ€™t just â€œmagic math.â€ Itâ€™s literally about rotating axes to capture the most variance. The math is heavy, but the intuition is powerful:
ğŸ‘‰ â€œFind the direction where the data spreads out the most, and project onto it.â€

Tomorrow, Iâ€™ll put this into practice on the MNIST dataset to see PCA in action on real image data.

Curious to hear:
What was the hardest ML math concept that finally clicked for you after struggling with it?

#MachineLearning #DataScience #PCA #AI #LearningInPublic
