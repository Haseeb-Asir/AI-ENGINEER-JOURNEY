ğŸš€ Day 120 of my AI Engineer Journey

Today I finally applied PCA (Principal Component Analysis) on a real dataset â€” the famous MNIST handwritten digits (0â€“9). âœï¸

Hereâ€™s what I discovered:

Number of components (n_components) matters a lot â†’ with just 2â€“3 components, accuracy was poor, but as I increased components, the model performance improved.

Learned about explained variance â†’ each eigenvalue (Î») tells how much variance its component captures. By dividing Î» by the total sum of Î»â€™s, we get the percentage of variance explained.

Used the cumulative sum rule â†’ keep components until 90% variance is explained. This helped me find the optimum number of principal components.

When PCA doesnâ€™t work â†’ circular data or cases where projection loses patterns. In such cases, we need other dimensionality reduction techniques.

ğŸ”‘ Lesson learned:
PCA isnâ€™t just for speed â€” itâ€™s for balancing dimensionality with information. Too few components = poor accuracy. Too many = no reduction. The real magic is in finding the sweet spot.

âœ¨ With this, Iâ€™ve officially wrapped up my feature engineering journey. From tomorrow, Iâ€™m finally diving into machine learning algorithms â€” the part Iâ€™ve been most excited about since Day 1. ğŸš€

ğŸ‘‰ If youâ€™ve used PCA before â€” how many components do you usually keep for your projects?

#MachineLearning #DataScience #PCA #AI #LearningInPublic
