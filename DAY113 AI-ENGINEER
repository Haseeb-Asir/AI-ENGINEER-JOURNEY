ğŸš€ Day 113 of my AI Engineer Journey

Todayâ€™s big focus â†’ Handling Missing Data (Categorical + Advanced Techniques)
And honestlyâ€¦ this step feels like the â€œsilent boss battleâ€ of ML. âš¡

Hereâ€™s what I learned (and tested on the Titanic dataset ğŸ›³):

1ï¸âƒ£ Mode / Most Frequent Imputation

Great when missingness is random & <5%

âœ… Simple, fast, effective for categorical

âŒ Can distort distribution

2ï¸âƒ£ Missing Category Imputation

Instead of filling â†’ create a new category called â€œMissingâ€

Best when data isnâ€™t missing at random & >5% gaps

3ï¸âƒ£ Random Imputation

Fill missing values with random samples from the same column

âœ… Preserves variance

âŒ Breaks correlation, memory-heavy

Works best with linear models, not decision trees

4ï¸âƒ£ Missing Indicator Method

Create a new binary column (True if missing, False if not)

Helps models capture the signal of missingness

5ï¸âƒ£ Automated Imputation (GridSearchCV)

ML itself finds the best imputation strategy

âœ… Removes guesswork

âŒ Computationally expensive

ğŸ’¡ Key takeaway:
Thereâ€™s no â€œone-size-fits-allâ€ for missing data. Sometimes the simplest fix works, sometimes you need to engineer missingness itself into a feature.

ğŸ‘‰ What I love about ML is this: even what you donâ€™t know (missing values) can become a feature that improves predictions.

Curious â†’ Which missing data strategy do you find most reliable in real-world projects?

#MachineLearning #DataScience #Python #AI #LearningInPublic
