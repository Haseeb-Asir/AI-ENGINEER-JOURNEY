âœ¨ ğ——ğ—®ğ˜† ğŸ­ğŸ°ğŸ® â€” Two new learnings today ğŸš€

ğŸ‘¨â€ğŸ’» DSA Journey (Started Today!)

 Dove into Algorithmic Complexity â€” the heart of efficient coding.

Time â±: How fast we solve problems (Google search as an example).

Space ğŸ’¾: How well we optimize memory (Facebook saving costs with smaller images).

Learned two ways to measure time:

 1ï¸âƒ£ Execution time (not reliable â€” hardware & inputs matter).

 2ï¸âƒ£ Counting operations (hardware-independent, clearer).

Tomorrow â†’ Iâ€™ll dive into abstraction & growth notations (Big-O).

ğŸ¤– ML Side

 Explored Lasso Regression (L1 Regularization) and Elastic Net:

Lasso â†’ creates sparsity, some coefficients shrink to zero â†’ helps in feature selection.

Elastic Net â†’ balances Lasso + Ridge, great for correlated features.

Key insight: Elastic Net gives flexibility with the L1_ratio hyperparameter.

ğŸ‘‰ Lesson: Whether in DSA or ML, efficiency matters â€” sometimes in time, sometimes in features.

â“What do you think is harder to optimize â€” time complexity in code or feature selection in ML?

#MachineLearning #DSA #LearningInPublic #AI #ProblemSolving
