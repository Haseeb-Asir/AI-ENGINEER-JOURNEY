ğŸš€ Day 115 of my AI Engineer Journey

Outliersâ€¦ the troublemakers of datasets. ğŸ“Š

Today I finally understood what they are, why they matter, and when theyâ€™re actually useful.

ğŸ‘‰ What I learned:

Outliers = data points that donâ€™t â€œfit in.â€ Example: everyone earns in thousands, one person earns in millions.

They can break models like Linear Regression, Logistic Regression, and Deep Learning (all weight-based).

But for tree-based models and anomaly detection, outliers are the heroes.

ğŸ‘‰ How to detect them:
1ï¸âƒ£ Normal distribution method â†’ Beyond Â±3Ïƒ (mean Â± 3 standard deviations).
2ï¸âƒ£ Boxplot/IQR method â†’ Works for skewed distributions (Q1 - 1.5Ã—IQR, Q3 + 1.5Ã—IQR).
3ï¸âƒ£ Percentile method â†’ Anything beyond the 1st or 99th percentile.

ğŸ‘‰ How to handle them:

Trimming â†’ Remove them (simple, but risk losing info).

Capping â†’ Limit extreme values to min/max thresholds.

ğŸ’¡ Lesson learned:
Outliers arenâ€™t always â€œbad data.â€ Sometimes theyâ€™re clues â€” pointing to anomalies, fraud, or rare but important cases. The key is knowing when to remove them vs. when to keep them.

I practiced on 3 datasets and saw how trimming/capping reshaped distributions. Some models got cleaner, but others actually lost signal.

ğŸ”‘ Fact that stuck with me:
Tree-based models donâ€™t care much about outliers â€” but linear ones do. That one insight alone makes you rethink preprocessing strategies.

ğŸ‘‰ How do you usually deal with outliers â€” trim them, cap them, or embrace them?

#MachineLearning #DataScience #AI #Python #LearningInPublic
