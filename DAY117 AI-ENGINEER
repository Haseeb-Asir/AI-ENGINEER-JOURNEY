ğŸš€ Day 117 of my AI Engineer Journey

Today, I finally got into Feature Extraction â€” the 4th step of Feature Engineering.
And the star of the show? ğŸ‘‰ PCA (Principal Component Analysis).

ğŸ’¡ What I learned today:

PCA is an unsupervised ML algorithm that reduces dimensionality but keeps the important info.

Think of it like a photographer ğŸ“¸ â€” capturing a 3D scene in just 2D, but still keeping the essence.

Example: If both rooms and washrooms are important features when predicting house price â†’ PCA can combine them into a new feature like â€œsizeâ€, instead of dropping one.

Key idea â†’ variance matters. The more variance a principal component captures, the more useful it is.

ğŸ‘‰ Why itâ€™s powerful:

Makes algorithms run faster âš¡

Helps us visualize data (since we canâ€™t see more than 3D).

Saves us from the curse of dimensionality â€” too many features = slower + less accurate models.

ğŸ¯ Lesson learned:
Sometimes you donâ€™t have to delete featuresâ€¦ you can compress and transform them into something more meaningful. PCA is like finding the perfect angle to look at your data.

ğŸ‘‰ Question for you:
Do you prefer Feature Selection (dropping irrelevant columns) or Feature Extraction (transforming them into new ones like PCA)?

#MachineLearning #FeatureEngineering #DataScience #AI #LearningInPublic
