ğŸš€ Day 116 of my AI Engineer Journey

Today was packed with Feature Engineering lessons â€” specifically Feature Construction and the Curse of Dimensionality.

ğŸ‘‰ What I did today:

Learned Feature Construction â†’ creating new features manually using domain knowledge.

Combined Titanic columns SibSp + Parch â†’ made a new column Family Type (0 = alone, 1 = family 1â€“4, 2 = family 4+).

Performed Feature Splitting â†’ split â€œNameâ€ column into Salutation + Name, which surprisingly boosted model accuracy by ~2%.

Studied the Curse of Dimensionality ğŸ“‰

Adding more features initially improves performance â†’ but after a point, accuracy stagnates or even decreases.

High-dimensional data (like images & text) causes sparsity â†’ data points get too spread out, making models slower & less accurate.

Example: MNIST dataset â†’ 28Ã—28 pixels = 784 features, but most pixels donâ€™t add value to recognizing the digit.

ğŸ‘‰ Lesson learned:

Feature construction is more of an art â€” no rules, just domain knowledge.

The curse of dimensionality is real â†’ more features â‰  better models.

Thatâ€™s why we need Dimensionality Reduction â†’ through Feature Selection (keep the best ones) or Feature Extraction (transform into fewer, more useful features).

ğŸ’¡ This balance between adding meaningful features and removing noise seems to be the real skill in ML.

ğŸ‘‰ Question for you:
When improving your models â€” do you lean more towards creating new features or reducing existing ones?

#MachineLearning #DataScience #FeatureEngineering #AI #LearningInPublic
